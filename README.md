# ğŸ“¸ Image Caption Generator

![Python](https://img.shields.io/badge/Python-3.10-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.10-orange)
![Gradio](https://img.shields.io/badge/Gradio-Deployed-green)

A Generative AI project that automatically describes the content of an image using Deep Learning. It combines **Computer Vision (InceptionV3)** and **Natural Language Processing (LSTM)** to generate accurate, human-like captions.

## ğŸš€ Live Demo
The model is deployed and running live! You can test it with your own images here:
**[ğŸ‘‰ Click here to try the Live App on Hugging Face](https://huggingface.co/spaces/RupamG/Image_Captioning_System)**

---

## ğŸ§  Technical Architecture
This project uses an **Encoder-Decoder** architecture:

1.  **Image Encoder (InceptionV3):**
    * We use a pre-trained InceptionV3 model (trained on ImageNet) to extract high-level visual features from images.
    * The last classification layer is removed, leaving us with a feature vector of shape `(2048,)`.
2.  **Sequence Decoder (LSTM):**
    * The extracted image features are passed to an LSTM (Long Short-Term Memory) network.
    * The LSTM learns to generate a sequence of words (caption) based on the image features and the previous words generated.

**Model Pipeline:**
`Input Image` â¡ï¸ `InceptionV3` â¡ï¸ `Feature Vector` â¡ï¸ `LSTM` â¡ï¸ `Predicted Caption`

---

## ğŸ“‚ Dataset
The model was trained on the **Flickr8k Dataset**, which consists of:
* **8,000 images** (6,000 training, 1,000 val, 1,000 test).
* **5 captions per image** (Total 40,000 captions).

*> **Note:** Due to size constraints, the raw dataset is not included in this repository. You can download it from [Kaggle](https://www.kaggle.com/adityajn105/flickr8k) and place it in the `src/` folder.*

---

## ğŸ› ï¸ Installation & Setup
To run this project locally on your machine:

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/Marshal-GG/Advanced-Image-Captioning-System.git](https://github.com/Marshal-GG/Advanced-Image-Captioning-System.git)
    cd Image-Captioning-System
    ```

2.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Download the Data:**
    * Download Flickr8k images and `Flickr8k.token.txt`.
    * Place them in the `src/` folder (or update paths in the notebook).

4.  **Run the Training Notebook:**
    * Open `notebooks/Image_Captioning_Training.ipynb` to see the data preprocessing, model training, and evaluation steps.

5.  **Run the App:**
    ```bash
    python app.py
    ```

---

## ğŸ“Š Results
* **Metric:** The model effectiveness is evaluated using qualitative analysis (visual inspection).
* **Sample Output:**
    * *Input:* Image of a dog running on grass.
    * 
    * *Output:* "A dog is running through the grass."

---

## ğŸ“ Directory Structure
â”œâ”€â”€ app.py # Gradio application for live demo 
â”œâ”€â”€ notebooks/ 
â”‚ â””â”€â”€ main.ipynb # Step-by-step training notebook 
â”œâ”€â”€ models/ 
â”‚ â””â”€â”€ trained_model.keras # Saved model (handled by Git LFS) 
â”œâ”€â”€ requirements.txt # List of python libraries used 
â”œâ”€â”€ vocab.pkl # Tokenizer vocabulary 
â”œâ”€â”€ wordtoix.pkl # Word-to-index mapping
â””â”€â”€ README.md # Project documentation

## ğŸ¤ Connect
If you have any questions about this project or want to discuss Generative AI, feel free to connect!
* **LinkedIn:** [https://www.linkedin.com/in/rupam-g/]
* **Email:** [marshalgcom@gmail.com]
